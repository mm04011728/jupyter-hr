{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_PAGE_NUMBER = 50\n",
    "KEYWORDS = [ 'python', '.net','数据挖掘','大数据','数据分析','推荐算法','机器学习','.net core']\n",
    "ADDRESS = ['北京', ]\n",
    "basic_url = 'http://sou.zhaopin.com/jobs/searchresult.ashx?'\n",
    "\n",
    "paras = {'jl': '北京','kw': '数据挖掘','p': '1'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from urllib.parse import urlencode\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from itertools import product\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:51.0) Gecko/20100101 Firefox/51.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    delay = random.randint(0, 2)\n",
    "    time.sleep(delay)  # 设置等待时间，防止网站反爬虫\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = basic_url + urlencode(paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(html):\n",
    "    # 记录保存日期\n",
    "    date = datetime.now().date()\n",
    "    date = datetime.strftime(date, '%Y-%m-%d')  # 转变成str\n",
    "\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    body = soup.body\n",
    "    data_main = body.find('div', {'class': 'newlist_list_content'})\n",
    "\n",
    "    if data_main:\n",
    "        tables = data_main.find_all('table')\n",
    "\n",
    "        for i, table_info in enumerate(tables):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            tds = table_info.find('tr').find_all('td')\n",
    "            zwmc = tds[0].find('a').get_text()  # 职位名称\n",
    "            zw_link = tds[0].find('a').get('href')  # 职位链接\n",
    "            fkl = tds[1].find('span').get_text()  # 反馈率\n",
    "            gsmc = tds[2].find('a').get_text()  # 公司名称\n",
    "            zwyx = tds[3].get_text()  # 职位月薪\n",
    "            gzdd = tds[4].get_text()  # 工作地点\n",
    "            gbsj = tds[5].find('span').get_text()  # 发布日期\n",
    "\n",
    "            tr_brief_spans = table_info.find(\n",
    "                'tr', {'class': 'newlist_tr_detail'}).find_all('span')\n",
    "\n",
    "            gsdd = \"\"\n",
    "            gsxz = \"\"\n",
    "            gsgm = \"\"\n",
    "            gzjy = \"\"\n",
    "            xlyq = \"\"\n",
    "            zwyx_1 = \"\"\n",
    "            for list in tr_brief_spans:\n",
    "                if len(list.get_text().split('：')) >= 2:\n",
    "                    if list.get_text().split('：')[0] == \"地点\":\n",
    "                        gsdd = list.get_text().split('：')[1]  # 公司地点\n",
    "                    elif list.get_text().split('：')[0] == \"公司性质\":\n",
    "                        gsxz = list.get_text().split('：')[1]  # 公司性质\n",
    "                    elif list.get_text().split('：')[0] == \"公司规模\":\n",
    "                        gsgm = list.get_text().split('：')[1]  # 公司规模\n",
    "                    elif list.get_text().split('：')[0] == \"经验\":\n",
    "                        gzjy = list.get_text().split('：')[1]  # 工作经验\n",
    "                    elif list.get_text().split('：')[0] == \"学历\":\n",
    "                        xlyq = list.get_text().split('：')[1]  # 学历要求\n",
    "                    elif list.get_text().split('：')[0] == \"职位月薪\":\n",
    "                        zwyx_1 = list.get_text().split('：')[1]  # 职位月薪(元/月)\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "\n",
    "            # 用生成器获取信息\n",
    "            yield {'职位名称': zwmc,  # 职位名称\n",
    "                   '反馈率': fkl,  # 反馈率\n",
    "                   '公司名称': gsmc,  # 公司名称\n",
    "                   '职位月薪': zwyx,  # 职位月薪\n",
    "                   '工作地点': gzdd,  # 工作地点\n",
    "                   '公布时间': gbsj,  # 公布时间\n",
    "                   '职位链接': zw_link,  # 网页链接\n",
    "                   '保存时间': date,  # 记录信息保存的日期\n",
    "                   '公司地点': gsdd,  # 公司地点\n",
    "                   '公司性质': gsxz,  # 公司性质\n",
    "                   '公司规模': gsgm,  # 公司规模\n",
    "                   '工作经验': gzjy,  # 工作经验\n",
    "                   '学历要求': xlyq,  # 学历要求\n",
    "                   '职业薪水': zwyx_1,  # 职位月薪(元/月)\n",
    "                   }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_between_re(start,end):\n",
    "    return '(?<={0}).*(?={1})'.format(start,end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_str(match_re):\n",
    "    if match_re:\n",
    "        return match_re.group(0)\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detal(html):\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    body = soup.body\n",
    "    data_main = body.find('div', {'class': 'tab-inner-cont'})\n",
    "    if data_main:\n",
    "        text = data_main.get_text()\n",
    "        to_del_re = '\\r|\\n|\\t| |查看职位地图'\n",
    "        text_clear = re.sub(to_del_re,'',text)\n",
    "#         print(text_clear)\n",
    "        first_re = '((职位要求(：|:))|(职位描述(：|:))|(岗位职责(：|:))|(岗位描述(：|:))|(工作职责(：|:)))'\n",
    "        second_re = '((岗位要求(：|:))|(职位要求(：|:))|(任职要求(：|:))|(任职资格(：|:))|(工作要求(：|:)))'\n",
    "        third_re = '(工作地址(：|:))'\n",
    "        responsibilities_search = re.search(get_between_re(first_re,second_re),text_clear)\n",
    "        if '工作地址' in text_clear:\n",
    "            requirements_search = re.search(get_between_re(second_re,third_re),text_clear)\n",
    "        else:\n",
    "            requirements_search = re.search('(?<={0}).*'.format(second_re),text_clear)\n",
    "        addr_search = re.search('(?<={0}).*'.format(third_re),text_clear)\n",
    "        responsibilities = get_search_str(responsibilities_search) #岗位职责\n",
    "        requirements = get_search_str(requirements_search) #技能要求\n",
    "        addr = get_search_str(addr_search) #公司地址\n",
    "        return(responsibilities,requirements,addr)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = 'http://jobs.zhaopin.com/120072290295717.htm'\n",
    "html1 = download(url1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_content(html)\n",
    "for each in data:\n",
    "    for k,v in each.items():\n",
    "        if k == '职位链接':\n",
    "            print(v)\n",
    "            html1 = download(v)\n",
    "            data1 = get_detal(html1)\n",
    "            for s in data1:\n",
    "                print(s)\n",
    "            print('\\r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
